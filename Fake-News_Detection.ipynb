{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('data/train.csv')\n",
    "test=pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...  \n",
       "1                      NaN  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...  \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...  \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \\\n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...   \n",
       "1                      NaN  Russian warships ready to strike terrorists ne...   \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...   \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...   \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...   \n",
       "\n",
       "  label  \n",
       "0     t  \n",
       "1     t  \n",
       "2     t  \n",
       "3     t  \n",
       "4     t  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label']='t'\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20800 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      "id        20800 non-null int64\n",
      "title     20242 non-null object\n",
      "author    18843 non-null object\n",
      "text      20761 non-null object\n",
      "label     20800 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5200 entries, 0 to 5199\n",
      "Data columns (total 5 columns):\n",
      "id        5200 non-null int64\n",
      "title     5078 non-null object\n",
      "author    4697 non-null object\n",
      "text      5193 non-null object\n",
      "label     5200 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 203.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data prep\n",
    "test=test.fillna(' ')\n",
    "train=train.fillna(' ')\n",
    "test['all']=test['title']+' '+test['author']+test['text']\n",
    "train['all']=train['title']+' '+train['author']+train['text']\n",
    "train['all']= [x.lower() for x in train['all']]\n",
    "train['all'] = train['all'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "test['all']= [x.lower() for x in test['all']]\n",
    "test['all'] = test['all'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "#tfidf\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "counts = count_vectorizer.fit_transform(train['all'].values)\n",
    "tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 and',\n",
       " '00 athletic',\n",
       " '00 crimes',\n",
       " '00 espanyol',\n",
       " '00 flour',\n",
       " '00 granada',\n",
       " '00 he',\n",
       " '00 jobs',\n",
       " '00 las',\n",
       " '00 mlaga',\n",
       " '00 mr',\n",
       " '00 new',\n",
       " '00 osasuna',\n",
       " '00 our',\n",
       " '00 percent',\n",
       " '00 prey',\n",
       " '00 real',\n",
       " '00 sevilla',\n",
       " '00 the',\n",
       " '00 utc',\n",
       " '00 villarreal',\n",
       " '00 zip',\n",
       " '000',\n",
       " '000 000',\n",
       " '000 015',\n",
       " '000 12',\n",
       " '000 120',\n",
       " '000 125',\n",
       " '000 20',\n",
       " '000 200',\n",
       " '000 2017',\n",
       " '000 24',\n",
       " '000 26',\n",
       " '000 30',\n",
       " '000 434',\n",
       " '000 50',\n",
       " '000 54',\n",
       " '000 55',\n",
       " '000 65',\n",
       " '000 650',\n",
       " '000 800',\n",
       " '000 900',\n",
       " '000 ____',\n",
       " '000 _____',\n",
       " '000 abandoned',\n",
       " '000 abortions',\n",
       " '000 about',\n",
       " '000 academics',\n",
       " '000 accomplishing',\n",
       " '000 according',\n",
       " '000 accounts',\n",
       " '000 acoustic',\n",
       " '000 acre',\n",
       " '000 acres',\n",
       " '000 across',\n",
       " '000 active',\n",
       " '000 actors',\n",
       " '000 acutely',\n",
       " '000 added',\n",
       " '000 adding',\n",
       " '000 additional',\n",
       " '000 addresses',\n",
       " '000 admissions',\n",
       " '000 adopted',\n",
       " '000 ads',\n",
       " '000 adult',\n",
       " '000 adults',\n",
       " '000 advance',\n",
       " '000 advocates',\n",
       " '000 affected',\n",
       " '000 afghan',\n",
       " '000 afghans',\n",
       " '000 african',\n",
       " '000 after',\n",
       " '000 afterwards',\n",
       " '000 against',\n",
       " '000 agency',\n",
       " '000 agents',\n",
       " '000 airline',\n",
       " '000 albert',\n",
       " '000 aliens',\n",
       " '000 alleyway',\n",
       " '000 allocated',\n",
       " '000 alternatives',\n",
       " '000 although',\n",
       " '000 altogether',\n",
       " '000 amatrice',\n",
       " '000 american',\n",
       " '000 americans',\n",
       " '000 among',\n",
       " '000 amtrak',\n",
       " '000 an',\n",
       " '000 ana',\n",
       " '000 and',\n",
       " '000 andrew',\n",
       " '000 anne',\n",
       " '000 annual',\n",
       " '000 annually',\n",
       " '000 another',\n",
       " '000 antipersonnel',\n",
       " '000 antique',\n",
       " '000 antitank',\n",
       " '000 apartments',\n",
       " '000 apiece',\n",
       " '000 applicants',\n",
       " '000 applications',\n",
       " '000 appointments',\n",
       " '000 approvals',\n",
       " '000 approved',\n",
       " '000 arabs',\n",
       " '000 are',\n",
       " '000 armed',\n",
       " '000 army',\n",
       " '000 arrest',\n",
       " '000 arrests',\n",
       " '000 arria',\n",
       " '000 arrived',\n",
       " '000 arriving',\n",
       " '000 article',\n",
       " '000 articles',\n",
       " '000 artwork',\n",
       " '000 artworks',\n",
       " '000 as',\n",
       " '000 asked',\n",
       " '000 assaults',\n",
       " '000 asylum',\n",
       " '000 at',\n",
       " '000 athletes',\n",
       " '000 attended',\n",
       " '000 attendees',\n",
       " '000 audi',\n",
       " '000 austin',\n",
       " '000 australian',\n",
       " '000 austrian',\n",
       " '000 autopsies',\n",
       " '000 available',\n",
       " '000 average',\n",
       " '000 award',\n",
       " '000 awr',\n",
       " '000 babies',\n",
       " '000 back',\n",
       " '000 backers',\n",
       " '000 backlash',\n",
       " '000 bail',\n",
       " '000 ballots',\n",
       " '000 bamboo',\n",
       " '000 bank',\n",
       " '000 bankrate',\n",
       " '000 baptism',\n",
       " '000 barely',\n",
       " '000 barrels',\n",
       " '000 based',\n",
       " '000 batteries',\n",
       " '000 battery',\n",
       " '000 be',\n",
       " '000 beach',\n",
       " '000 bedouins',\n",
       " '000 beekeepers',\n",
       " '000 beers',\n",
       " '000 before',\n",
       " '000 beijings',\n",
       " '000 bereitstehen',\n",
       " '000 between',\n",
       " '000 bikes',\n",
       " '000 bill',\n",
       " '000 billion',\n",
       " '000 bills',\n",
       " '000 births',\n",
       " '000 black',\n",
       " '000 blacks',\n",
       " '000 blood',\n",
       " '000 blue',\n",
       " '000 boats',\n",
       " '000 bolvars',\n",
       " '000 bombers',\n",
       " '000 bond',\n",
       " '000 bonus',\n",
       " '000 book',\n",
       " '000 books',\n",
       " '000 boots',\n",
       " '000 border',\n",
       " '000 borrowers',\n",
       " '000 both',\n",
       " '000 bottles',\n",
       " '000 boxes',\n",
       " '000 brains',\n",
       " '000 breast',\n",
       " '000 breeding',\n",
       " '000 brian',\n",
       " '000 bringing',\n",
       " '000 british',\n",
       " '000 britons',\n",
       " '000 budget',\n",
       " '000 buildings',\n",
       " '000 bulbs',\n",
       " '000 businesses',\n",
       " '000 businesspeople',\n",
       " '000 but',\n",
       " '000 by',\n",
       " '000 californians',\n",
       " '000 calls',\n",
       " '000 calories',\n",
       " '000 came',\n",
       " '000 can',\n",
       " '000 canadian',\n",
       " '000 cancer',\n",
       " '000 canning',\n",
       " '000 captive',\n",
       " '000 captives',\n",
       " '000 car',\n",
       " '000 cardiac',\n",
       " '000 cards',\n",
       " '000 career',\n",
       " '000 carrier',\n",
       " '000 cars',\n",
       " '000 cases',\n",
       " '000 cash',\n",
       " '000 category',\n",
       " '000 catholics',\n",
       " '000 cattle',\n",
       " '000 ceiling',\n",
       " '000 center',\n",
       " '000 central',\n",
       " '000 centrifuges',\n",
       " '000 certified',\n",
       " '000 cfs',\n",
       " '000 charges',\n",
       " '000 charlie',\n",
       " '000 charter',\n",
       " '000 check',\n",
       " '000 children',\n",
       " '000 chinese',\n",
       " '000 christians',\n",
       " '000 christies',\n",
       " '000 chunk',\n",
       " '000 cities',\n",
       " '000 citizens',\n",
       " '000 civil',\n",
       " '000 civilian',\n",
       " '000 civilians',\n",
       " '000 classified',\n",
       " '000 clients',\n",
       " '000 clips',\n",
       " '000 close',\n",
       " '000 coal',\n",
       " '000 college',\n",
       " '000 colleges',\n",
       " '000 combat',\n",
       " '000 combinations',\n",
       " '000 coming',\n",
       " '000 commemorative',\n",
       " '000 comments',\n",
       " '000 commercial',\n",
       " '000 commission',\n",
       " '000 common',\n",
       " '000 communities',\n",
       " '000 commuters',\n",
       " '000 companies',\n",
       " '000 compared',\n",
       " '000 complaints',\n",
       " '000 computer',\n",
       " '000 computers',\n",
       " '000 concrete',\n",
       " '000 concurrent',\n",
       " '000 condoms',\n",
       " '000 confirmations',\n",
       " '000 confiscated',\n",
       " '000 conflict',\n",
       " '000 connected',\n",
       " '000 connecticut',\n",
       " '000 constables',\n",
       " '000 construction',\n",
       " '000 contacts',\n",
       " '000 content',\n",
       " '000 contingent',\n",
       " '000 contract',\n",
       " '000 contribution',\n",
       " '000 convicted',\n",
       " '000 convictions',\n",
       " '000 convicts',\n",
       " '000 copies',\n",
       " '000 cops',\n",
       " '000 corcoran',\n",
       " '000 corporations',\n",
       " '000 cost',\n",
       " '000 could',\n",
       " '000 counterfeit',\n",
       " '000 counties',\n",
       " '000 coupon',\n",
       " '000 court',\n",
       " '000 creating',\n",
       " '000 credit',\n",
       " '000 criminal',\n",
       " '000 criminals',\n",
       " '000 crooked',\n",
       " '000 cuban',\n",
       " '000 cubans',\n",
       " '000 cubic',\n",
       " '000 cumulatively',\n",
       " '000 current',\n",
       " '000 currently',\n",
       " '000 customer',\n",
       " '000 customers',\n",
       " '000 cut',\n",
       " '000 cuts',\n",
       " '000 daily',\n",
       " '000 dams',\n",
       " '000 daniel',\n",
       " '000 day',\n",
       " '000 days',\n",
       " '000 db11',\n",
       " '000 dead',\n",
       " '000 deaths',\n",
       " '000 debt',\n",
       " '000 decade',\n",
       " '000 deductible',\n",
       " '000 deductibles',\n",
       " '000 deer',\n",
       " '000 defective',\n",
       " '000 defendants',\n",
       " '000 degrees',\n",
       " '000 delegates',\n",
       " '000 deleted',\n",
       " '000 delirious',\n",
       " '000 democratic',\n",
       " '000 demonstrators',\n",
       " '000 depending',\n",
       " '000 deployed',\n",
       " '000 deportation',\n",
       " '000 deportations',\n",
       " '000 deposit',\n",
       " '000 deposits',\n",
       " '000 detained',\n",
       " '000 did',\n",
       " '000 didnt',\n",
       " '000 die',\n",
       " '000 died',\n",
       " '000 diesel',\n",
       " '000 different',\n",
       " '000 dig',\n",
       " '000 digital',\n",
       " '000 dinner',\n",
       " '000 dinners',\n",
       " '000 diplomatic',\n",
       " '000 disciplinary',\n",
       " '000 discrepancy',\n",
       " '000 dislikes',\n",
       " '000 displaced',\n",
       " '000 divorces',\n",
       " '000 dlares',\n",
       " '000 doctors',\n",
       " '000 documents',\n",
       " '000 dollars',\n",
       " '000 donated',\n",
       " '000 donation',\n",
       " '000 donations',\n",
       " '000 donors',\n",
       " '000 dont',\n",
       " '000 doorknob',\n",
       " '000 doses',\n",
       " '000 down',\n",
       " '000 dreamers',\n",
       " '000 dresses',\n",
       " '000 drivers',\n",
       " '000 drop',\n",
       " '000 drug',\n",
       " '000 during',\n",
       " '000 dutch',\n",
       " '000 dying',\n",
       " '000 each',\n",
       " '000 early',\n",
       " '000 egyptians',\n",
       " '000 egypts',\n",
       " '000 eight',\n",
       " '000 electric',\n",
       " '000 elephants',\n",
       " '000 email',\n",
       " '000 emails',\n",
       " '000 embraces',\n",
       " '000 emergency',\n",
       " '000 emigrants',\n",
       " '000 employees',\n",
       " '000 end',\n",
       " '000 engineering',\n",
       " '000 engineers',\n",
       " '000 enough',\n",
       " '000 entered',\n",
       " '000 enthusiasts',\n",
       " '000 established',\n",
       " '000 estate',\n",
       " '000 estimate',\n",
       " '000 eu',\n",
       " '000 euro',\n",
       " '000 european',\n",
       " '000 europeans',\n",
       " '000 euros',\n",
       " '000 even',\n",
       " '000 events',\n",
       " '000 eventually',\n",
       " '000 examples',\n",
       " '000 excluding',\n",
       " '000 existing',\n",
       " '000 expected',\n",
       " '000 exploded',\n",
       " '000 extra',\n",
       " '000 extremists',\n",
       " '000 fact',\n",
       " '000 factories',\n",
       " '000 factory',\n",
       " '000 faculty',\n",
       " '000 failed',\n",
       " '000 fairey',\n",
       " '000 fake',\n",
       " '000 families',\n",
       " '000 family',\n",
       " '000 famous',\n",
       " '000 fan',\n",
       " '000 fans',\n",
       " '000 farm',\n",
       " '000 farmers',\n",
       " '000 farms',\n",
       " '000 fatalities',\n",
       " '000 favor',\n",
       " '000 favorite',\n",
       " '000 federal',\n",
       " '000 fee',\n",
       " '000 feet',\n",
       " '000 fell',\n",
       " '000 felons',\n",
       " '000 female',\n",
       " '000 females',\n",
       " '000 fentanyl',\n",
       " '000 fewer',\n",
       " '000 fighters',\n",
       " '000 fighting',\n",
       " '000 fiji',\n",
       " '000 files',\n",
       " '000 films',\n",
       " '000 fine',\n",
       " '000 fines',\n",
       " '000 finnish',\n",
       " '000 first',\n",
       " '000 fled',\n",
       " '000 flight',\n",
       " '000 flocked',\n",
       " '000 flood',\n",
       " '000 florida',\n",
       " '000 flytraps',\n",
       " '000 follow',\n",
       " '000 followed',\n",
       " '000 followers',\n",
       " '000 food',\n",
       " '000 foot',\n",
       " '000 for',\n",
       " '000 force',\n",
       " '000 foreclosed',\n",
       " '000 foreclosures',\n",
       " '000 foreign',\n",
       " '000 foreigners',\n",
       " '000 former',\n",
       " '000 fqhcs',\n",
       " '000 fracking',\n",
       " '000 francesco',\n",
       " '000 francs',\n",
       " '000 french',\n",
       " '000 from',\n",
       " '000 full',\n",
       " '000 fund',\n",
       " '000 funds',\n",
       " '000 future',\n",
       " '000 gallons',\n",
       " '000 gambians',\n",
       " '000 game',\n",
       " '000 games',\n",
       " '000 gap',\n",
       " '000 garanganga',\n",
       " '000 garments',\n",
       " '000 gas',\n",
       " '000 gathered',\n",
       " '000 german',\n",
       " '000 germans',\n",
       " '000 get',\n",
       " '000 giddy',\n",
       " '000 gift',\n",
       " '000 give',\n",
       " '000 goldmans',\n",
       " '000 got',\n",
       " '000 government',\n",
       " '000 grade',\n",
       " '000 graduates',\n",
       " '000 grams',\n",
       " '000 grant',\n",
       " '000 greek',\n",
       " '000 green',\n",
       " '000 greenstreets',\n",
       " '000 grew',\n",
       " '000 grocery',\n",
       " '000 groups',\n",
       " '000 guests',\n",
       " '000 guns',\n",
       " '000 habitants',\n",
       " '000 had',\n",
       " '000 haitian',\n",
       " '000 haitians',\n",
       " '000 hamilton',\n",
       " '000 hardcover',\n",
       " '000 harvest',\n",
       " '000 has',\n",
       " '000 have',\n",
       " '000 having',\n",
       " '000 he',\n",
       " '000 head',\n",
       " '000 headsets',\n",
       " '000 headteachers',\n",
       " '000 health',\n",
       " '000 healthy',\n",
       " '000 hearings',\n",
       " '000 heartbroken',\n",
       " '000 hectares',\n",
       " '000 hemorrhaging',\n",
       " '000 her',\n",
       " '000 high',\n",
       " '000 higher',\n",
       " '000 hires',\n",
       " '000 hiroshima',\n",
       " '000 his',\n",
       " '000 hispanics',\n",
       " '000 hits',\n",
       " '000 hobbyists',\n",
       " '000 home',\n",
       " '000 homeless',\n",
       " '000 homes',\n",
       " '000 homicides',\n",
       " '000 hondurans',\n",
       " '000 hooded',\n",
       " '000 horses',\n",
       " '000 hosszu',\n",
       " '000 hotel',\n",
       " '000 hours',\n",
       " '000 households',\n",
       " '000 houses',\n",
       " '000 housing',\n",
       " '000 huge',\n",
       " '000 human',\n",
       " '000 ice',\n",
       " '000 if',\n",
       " '000 ill',\n",
       " '000 illegal',\n",
       " '000 illegally',\n",
       " '000 illegals',\n",
       " '000 im',\n",
       " '000 images',\n",
       " '000 imagine',\n",
       " '000 immigrants',\n",
       " '000 immigration',\n",
       " '000 impersonating',\n",
       " '000 in',\n",
       " '000 incidents',\n",
       " '000 included',\n",
       " '000 includes',\n",
       " '000 including',\n",
       " '000 income',\n",
       " '000 increase',\n",
       " '000 indian',\n",
       " '000 indians',\n",
       " '000 indigenous',\n",
       " '000 individual',\n",
       " '000 individually',\n",
       " '000 individuals',\n",
       " '000 infections',\n",
       " '000 inhabitants',\n",
       " '000 initiation',\n",
       " '000 inmates',\n",
       " '000 innocent',\n",
       " '000 inside',\n",
       " '000 insiders',\n",
       " '000 instagram',\n",
       " '000 installation',\n",
       " '000 instead',\n",
       " '000 insurance',\n",
       " '000 intellectual',\n",
       " '000 interested',\n",
       " '000 interlinked',\n",
       " '000 interlocking',\n",
       " '000 international',\n",
       " '000 internet',\n",
       " '000 interviews',\n",
       " '000 into',\n",
       " '000 investment',\n",
       " '000 investments',\n",
       " '000 ippolito',\n",
       " '000 iranians',\n",
       " '000 iraqi',\n",
       " '000 iraqis',\n",
       " '000 irish',\n",
       " '000 is',\n",
       " '000 iskmpfer',\n",
       " '000 islamic',\n",
       " '000 israelis',\n",
       " '000 issue',\n",
       " '000 it',\n",
       " '000 items',\n",
       " '000 its',\n",
       " '000 jacopo',\n",
       " '000 january',\n",
       " '000 japanese',\n",
       " '000 jean',\n",
       " '000 jewish',\n",
       " '000 jews',\n",
       " '000 jihadis',\n",
       " '000 jihadists',\n",
       " '000 jobs',\n",
       " '000 john',\n",
       " '000 joined',\n",
       " '000 jointly',\n",
       " '000 joules',\n",
       " '000 journalists',\n",
       " '000 juan',\n",
       " '000 jubilant',\n",
       " '000 judges',\n",
       " '000 just',\n",
       " '000 kalashnikov',\n",
       " '000 kansans',\n",
       " '000 kenny',\n",
       " '000 kentucky',\n",
       " '000 kids',\n",
       " '000 killings',\n",
       " '000 kilometers',\n",
       " '000 kilometres',\n",
       " '000 kinds',\n",
       " '000 km',\n",
       " '000 known',\n",
       " '000 kokang',\n",
       " '000 korean',\n",
       " '000 kosovars',\n",
       " '000 kurdish',\n",
       " '000 kushners',\n",
       " '000 kwacha',\n",
       " '000 laborers',\n",
       " '000 lamps',\n",
       " '000 largely',\n",
       " '000 last',\n",
       " '000 latinos',\n",
       " '000 law',\n",
       " '000 lawsuit',\n",
       " '000 lawyers',\n",
       " '000 leading',\n",
       " '000 leaving',\n",
       " '000 left',\n",
       " '000 lenders',\n",
       " '000 lending',\n",
       " '000 less',\n",
       " '000 letters',\n",
       " '000 level',\n",
       " '000 liberians',\n",
       " '000 libya',\n",
       " '000 licensees',\n",
       " '000 lien',\n",
       " '000 light',\n",
       " '000 lightyears',\n",
       " '000 likely',\n",
       " '000 likes',\n",
       " '000 limit',\n",
       " '000 line',\n",
       " '000 liquid',\n",
       " '000 liquidating',\n",
       " '000 liters',\n",
       " '000 little',\n",
       " '000 live',\n",
       " '000 lives',\n",
       " '000 living',\n",
       " '000 loan',\n",
       " '000 loans',\n",
       " '000 local',\n",
       " '000 locals',\n",
       " '000 locations',\n",
       " '000 locks',\n",
       " '000 long',\n",
       " '000 longshoremen',\n",
       " '000 loss',\n",
       " '000 lost',\n",
       " '000 macbook',\n",
       " '000 machines',\n",
       " '000 macomb',\n",
       " '000 madrasas',\n",
       " '000 making',\n",
       " '000 mann',\n",
       " '000 manufacturing',\n",
       " '000 many',\n",
       " '000 marchers',\n",
       " '000 marines',\n",
       " '000 mark',\n",
       " '000 marriages',\n",
       " '000 martyrs',\n",
       " '000 massachusetts',\n",
       " '000 massive',\n",
       " '000 materialized',\n",
       " '000 mattress',\n",
       " '000 meals',\n",
       " '000 means',\n",
       " '000 megawatts',\n",
       " '000 members',\n",
       " '000 membership',\n",
       " '000 men',\n",
       " '000 mennesker',\n",
       " '000 mennonites',\n",
       " '000 merchants',\n",
       " '000 meters',\n",
       " '000 metropolitan',\n",
       " '000 mexican',\n",
       " '000 microwave',\n",
       " '000 might',\n",
       " '000 migrant',\n",
       " '000 migrants',\n",
       " '000 mile',\n",
       " '000 miles',\n",
       " '000 miliardi',\n",
       " '000 militants',\n",
       " '000 military',\n",
       " '000 militiamen',\n",
       " '000 millardos',\n",
       " '000 milliards',\n",
       " '000 milligram',\n",
       " '000 million',\n",
       " '000 millones',\n",
       " '000 milyar',\n",
       " '000 miners',\n",
       " '000 misdemeanor',\n",
       " '000 missiles',\n",
       " '000 missing',\n",
       " '000 mistake',\n",
       " '000 mod',\n",
       " '000 monarch',\n",
       " '000 monday',\n",
       " '000 month',\n",
       " '000 monthly',\n",
       " '000 more',\n",
       " '000 mortgage',\n",
       " '000 mortgages',\n",
       " '000 mosques',\n",
       " '000 most',\n",
       " '000 mostly',\n",
       " '000 mountain',\n",
       " '000 mourners',\n",
       " '000 mr',\n",
       " '000 mrs',\n",
       " '000 ms',\n",
       " '000 much',\n",
       " '000 murdochs',\n",
       " '000 murres',\n",
       " '000 music',\n",
       " '000 muslim',\n",
       " '000 muslims',\n",
       " '000 myterm',\n",
       " '000 mythology',\n",
       " '000 names',\n",
       " '000 nanotechnology',\n",
       " '000 national',\n",
       " '000 nationwide',\n",
       " '000 native',\n",
       " '000 navajo',\n",
       " '000 near',\n",
       " '000 nearly',\n",
       " '000 neighbouring',\n",
       " '000 nest',\n",
       " '000 net',\n",
       " '000 neu',\n",
       " '000 new',\n",
       " '000 newly',\n",
       " '000 nicoletto',\n",
       " '000 night',\n",
       " '000 no',\n",
       " '000 nobel',\n",
       " '000 nonprofit',\n",
       " '000 north',\n",
       " '000 northpointe',\n",
       " '000 not',\n",
       " '000 now',\n",
       " '000 nuclear',\n",
       " '000 number',\n",
       " '000 nuns',\n",
       " '000 nursing',\n",
       " '000 nyclass',\n",
       " '000 obama',\n",
       " '000 objects',\n",
       " '000 observed',\n",
       " '000 occasions',\n",
       " '000 of',\n",
       " '000 offenses',\n",
       " '000 officer',\n",
       " '000 officers',\n",
       " '000 officials',\n",
       " '000 ohio',\n",
       " '000 oil',\n",
       " '000 old',\n",
       " '000 older',\n",
       " '000 olympic',\n",
       " '000 omaha',\n",
       " '000 on',\n",
       " '000 once',\n",
       " '000 one',\n",
       " '000 online',\n",
       " '000 only',\n",
       " '000 open',\n",
       " '000 or',\n",
       " '000 ordered',\n",
       " '000 organizers',\n",
       " '000 osstes',\n",
       " '000 other',\n",
       " '000 others',\n",
       " '000 oulus',\n",
       " '000 out',\n",
       " '000 outsourcing',\n",
       " '000 over',\n",
       " '000 overall',\n",
       " '000 overseeing',\n",
       " '000 overtaking',\n",
       " '000 owners',\n",
       " '000 packages',\n",
       " '000 page',\n",
       " '000 pages',\n",
       " '000 paid',\n",
       " '000 painted',\n",
       " '000 painting',\n",
       " '000 paintings',\n",
       " '000 paired',\n",
       " '000 pairs',\n",
       " '000 palestinian',\n",
       " '000 palestinians',\n",
       " '000 palm',\n",
       " '000 paper',\n",
       " '000 parisians',\n",
       " '000 part',\n",
       " '000 participants',\n",
       " '000 parts',\n",
       " '000 passengers',\n",
       " '000 passports',\n",
       " '000 patek',\n",
       " '000 pater',\n",
       " '000 patients',\n",
       " '000 pay',\n",
       " '000 payment',\n",
       " '000 payout',\n",
       " '000 pedestrians',\n",
       " '000 penalty',\n",
       " '000 pending',\n",
       " '000 people',\n",
       " '000 per',\n",
       " '000 percent',\n",
       " '000 personal',\n",
       " '000 personnel',\n",
       " '000 pesos',\n",
       " '000 photographs',\n",
       " '000 physician',\n",
       " '000 pieces',\n",
       " '000 pier',\n",
       " '000 pierre',\n",
       " '000 pill',\n",
       " '000 pipe',\n",
       " '000 placed',\n",
       " '000 places',\n",
       " '000 plaintiffs',\n",
       " '000 plan',\n",
       " '000 plane',\n",
       " '000 planners',\n",
       " '000 plant',\n",
       " '000 platelet',\n",
       " '000 players',\n",
       " '000 plus',\n",
       " '000 podcasts',\n",
       " '000 podestas',\n",
       " '000 points',\n",
       " '000 poles',\n",
       " '000 police',\n",
       " '000 political',\n",
       " '000 polling',\n",
       " '000 por',\n",
       " '000 portfolio',\n",
       " '000 portion',\n",
       " '000 positions',\n",
       " '000 postal',\n",
       " '000 posts',\n",
       " '000 potential',\n",
       " '000 pound',\n",
       " '000 pounds',\n",
       " '000 ppm',\n",
       " '000 preborn',\n",
       " '000 pregnancy',\n",
       " '000 pregnant',\n",
       " '000 premature',\n",
       " '000 previously',\n",
       " '000 prime',\n",
       " '000 print',\n",
       " '000 prison',\n",
       " '000 prisoners',\n",
       " '000 prize',\n",
       " '000 prizes',\n",
       " '000 processors',\n",
       " '000 procured',\n",
       " '000 produced',\n",
       " '000 professionals',\n",
       " '000 protested',\n",
       " '000 protesters',\n",
       " '000 protests',\n",
       " '000 public',\n",
       " '000 pumpen',\n",
       " '000 pupils',\n",
       " '000 put',\n",
       " '000 quadrupling',\n",
       " '000 qualified',\n",
       " '000 question',\n",
       " '000 questions',\n",
       " '000 quicken',\n",
       " '000 radioactive',\n",
       " '000 raise',\n",
       " '000 raised',\n",
       " '000 range',\n",
       " '000 rangers',\n",
       " '000 ransom',\n",
       " '000 rape',\n",
       " '000 rare',\n",
       " '000 rather',\n",
       " '000 reach',\n",
       " '000 readers',\n",
       " '000 real',\n",
       " '000 reals',\n",
       " '000 rebel',\n",
       " '000 rebels',\n",
       " '000 receive',\n",
       " '000 received',\n",
       " '000 reconstructive',\n",
       " '000 recordings',\n",
       " '000 records',\n",
       " '000 red',\n",
       " '000 refrigerators',\n",
       " '000 refugees',\n",
       " '000 registered',\n",
       " '000 relatives',\n",
       " '000 religious',\n",
       " '000 remain',\n",
       " '000 remaining',\n",
       " '000 remarkably',\n",
       " '000 renminbi',\n",
       " '000 repayment',\n",
       " '000 replied',\n",
       " '000 reported',\n",
       " '000 reports',\n",
       " '000 representatives',\n",
       " '000 republicans',\n",
       " '000 required',\n",
       " '000 rescued',\n",
       " '000 research',\n",
       " '000 reserve',\n",
       " '000 residents',\n",
       " '000 residing',\n",
       " '000 resistance',\n",
       " '000 respondents',\n",
       " '000 responses',\n",
       " '000 restaurants',\n",
       " '000 retail',\n",
       " '000 returning',\n",
       " '000 revenues',\n",
       " '000 reward',\n",
       " '000 riders',\n",
       " '000 rides',\n",
       " '000 robberies',\n",
       " '000 rohingya',\n",
       " '000 role',\n",
       " '000 roman',\n",
       " '000 roof',\n",
       " '000 roubles',\n",
       " '000 rounds',\n",
       " '000 runners',\n",
       " '000 rupee',\n",
       " '000 rupees',\n",
       " '000 rural',\n",
       " '000 rushing',\n",
       " '000 russian',\n",
       " '000 russians',\n",
       " '000 said',\n",
       " '000 salaam',\n",
       " '000 salary',\n",
       " '000 sales',\n",
       " '000 samples',\n",
       " '000 samsung',\n",
       " '000 satellites',\n",
       " '000 savings',\n",
       " '000 say',\n",
       " '000 saying',\n",
       " '000 scales',\n",
       " '000 scholarship',\n",
       " '000 schools',\n",
       " '000 schwarzenegger',\n",
       " '000 screens',\n",
       " '000 sculptures',\n",
       " '000 sea',\n",
       " '000 seafood',\n",
       " '000 searchable',\n",
       " '000 seats',\n",
       " ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = train['label'].values\n",
    "\n",
    "#split in samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NB  classifier on training set: 0.87\n",
      "Accuracy of NB classifier on test set: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "\n",
    "NB = MultinomialNB()\n",
    "NB.fit(X_train, y_train)\n",
    "print('Accuracy of NB  classifier on training set: {:.2f}'\n",
    "     .format(NB.score(X_train, y_train)))\n",
    "print('Accuracy of NB classifier on test set: {:.2f}'\n",
    "     .format(NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3622623"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  del sys.path[0]\n",
      "c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, recurrent_dropout=0.2, dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 3622623, 128)      128000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 383,194\n",
      "Trainable params: 383,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, embed_dim,input_length = tfidf.shape[1], dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720, 3622623) (18720, 2)\n",
      "(2080, 3622623) (2080, 2)\n"
     ]
    }
   ],
   "source": [
    "targets = pd.get_dummies(train['label']).values\n",
    "\n",
    "#split in samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0, test_size=0.10)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20,3622623,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[Node: embedding_3/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/embedding_3/GatherV2_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3/Cast, lstm_3/TensorArrayUnstack/range/start)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'embedding_3/GatherV2', defined at:\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-76-fe653f06f85b>\", line 13, in <module>\n    model.add(Embedding(1000, embed_dim,input_length = tfidf.shape[1], dropout = 0.2))\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 497, in add\n    layer(x)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1215, in gather\n    return tf.gather(reference, indices)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2736, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3668, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,3622623,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[Node: embedding_3/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/embedding_3/GatherV2_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3/Cast, lstm_3/TensorArrayUnstack/range/start)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,3622623,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[Node: embedding_3/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/embedding_3/GatherV2_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3/Cast, lstm_3/TensorArrayUnstack/range/start)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-6d66720ddc14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Score: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation Accuracy: %.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20,3622623,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[Node: embedding_3/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/embedding_3/GatherV2_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3/Cast, lstm_3/TensorArrayUnstack/range/start)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'embedding_3/GatherV2', defined at:\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-76-fe653f06f85b>\", line 13, in <module>\n    model.add(Embedding(1000, embed_dim,input_length = tfidf.shape[1], dropout = 0.2))\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\", line 497, in add\n    layer(x)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\embeddings.py\", line 138, in call\n    out = K.gather(self.embeddings, inputs)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1215, in gather\n    return tf.gather(reference, indices)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2736, in gather\n    return gen_array_ops.gather_v2(params, indices, axis, name=name)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3668, in gather_v2\n    \"GatherV2\", params=params, indices=indices, axis=axis, name=name)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"c:\\users\\crist\\appdata\\local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[20,3622623,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[Node: embedding_3/GatherV2 = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@training_2/Adam/gradients/embedding_3/GatherV2_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](embedding_3/embeddings/read, embedding_3/Cast, lstm_3/TensorArrayUnstack/range/start)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = batch_size, nb_epoch = 1, verbose = 2)\n",
    "\n",
    "score,acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Validation Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
